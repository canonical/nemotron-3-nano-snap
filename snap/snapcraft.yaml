name: nemotron-3-nano
base: core24
summary: Inference Snap for Nemotron 3 nano
description: |
  This is an inference snap that lets you run Nemotron 3 nano.

adopt-info: version

grade: devel
confinement: strict
compression: lzo

environment:
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION
  ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR

plugs:
  # To allow side-loading models into server (root daemon) from user home directory
  home:
    read: all

hooks:
  install:
    plugs:
      - hardware-observe
      - opengl

parts:
  version:
    plugin: dump
    source: .
    build-packages:
      - git-lfs
    override-pull: |
      craftctl set version="v3+$(git -C $SNAPCRAFT_PROJECT_DIR describe --always)"

  cli:
    source: https://github.com/canonical/inference-snaps-cli.git
    source-tag: v1.0.0-beta.27
    plugin: go
    build-snaps:
      - go/1.24/stable
    stage-packages:
      - pciutils
      - clinfo
    override-build: |
      craftctl default

      # For tab completion
      ln --symbolic ./modelctl $CRAFT_PART_INSTALL/bin/nemotron-3-nano
    organize:
      bin/cli: bin/modelctl
        
  engines:
    source: engines
    plugin: dump
    organize:
      "*": engines/
  
  scripts:
    source: scripts
    plugin: dump
    organize:
      "*": bin/

  common-runtime-dependencies:
    plugin: nil
    stage-snaps:
      - jq

  model-30b-a3b-q4-k-m-gguf:
    plugin: nil
    override-prime: |
      set +x # reduce noise

      # Bypassing the usual part lifecycle saves disk space and time when dealing with large files
      # Copy model files directly from the project into the respective component's prime directory
      source="$CRAFT_PROJECT_DIR/components/model-30b-a3b-q4-k-m-gguf"
      target="CRAFT_COMPONENT_MODEL_30B_A3B_Q4_K_M_GGUF_PRIME"

      cp -v $source/* ${!target}/

  llama-cpp:
    plugin: dump
    source: 
      - on amd64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7731/llamacpp-amd64.tar.gz
      - on arm64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7731/llamacpp-arm64.tar.gz
    override-build: |
      # Move license files to be included in the snap, not the component
      mkdir -p $CRAFT_PRIME/usr/share/doc/
      mv licenses $CRAFT_PRIME/usr/share/doc/llama.cpp

      craftctl default
    stage-packages:
      - libgomp1
    organize:
      "*": (component/llama-cpp)
  
  llama-cpp-local-files:
    plugin: dump
    source: components/llama-cpp
    organize:
      "*": (component/llama-cpp)
    
  llama-cpp-cuda:
    plugin: dump
    source: 
      - on amd64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7731/llamacpp-amd64+cuda12.tar.gz
      - on arm64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7731/llamacpp-arm64+cuda12.tar.gz
    override-build: |
      # Move license files to be included in the snap, not the component
      mkdir -p $CRAFT_PRIME/usr/share/doc/
      mv licenses $CRAFT_PRIME/usr/share/doc/llama.cpp-cuda

      craftctl default
    stage-packages:
      - libgomp1
    organize:
      "*": (component/llama-cpp-cuda)

  llama-cpp-cuda-local-files:
    plugin: dump
    source: components/llama-cpp-cuda
    organize:
      "*": (component/llama-cpp-cuda)

  notice:
    plugin: nil
    source: NOTICE
    source-type: file
    override-build: |
      license_dir=$CRAFT_PART_INSTALL/usr/share/doc
      mkdir -p $license_dir
      cp NOTICE $license_dir/
      # Add license files referenced in NOTICE
      cp $SNAPCRAFT_PROJECT_DIR/LICENSE $license_dir/
      cp $SNAPCRAFT_PROJECT_DIR/nvidia-open-model-license-agreements-24-10-2025.pdf $license_dir/

components:
  model-30b-a3b-q4-k-m-gguf:
    type: standard
    summary: NVIDIA Nemotron-Nano-3-30B-A3B-Q4_K_M GGUF
    description: Model weights for NVIDIA Nemotron 3 Nano with 30B (3.5B active) parameters in GGUF format

  llama-cpp:
    type: standard
    summary: llama.cpp using default CPU instruction sets
    description: LLM inference in C/C++
  
  llama-cpp-cuda:
    type: standard
    summary: llama.cpp with CUDA backend
    description: LLM inference in C/C++

apps:
  nemotron-3-nano:
    command: bin/nemotron-3-nano
    completer: bin/completion.bash
    plugs:
      - hardware-observe
      - opengl
      - network

  server:
    command: bin/server.sh
    daemon: simple
    plugs:
      - network-bind
      - hardware-observe
      - opengl
      - home

