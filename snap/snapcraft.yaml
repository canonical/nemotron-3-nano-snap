name: nemotron-snap # you probably want to 'snapcraft register <name>'
base: core24 # the base snap is the execution environment for this snap
version: '0.0' # just for humans, typically '1.2+git' or '1.3.2'
summary: Inference Snap for NemotronNano330BA3B  # 79 char long summary
description: |
  This is an inference snap that lets you run NemotronNano330BA3B model using CPU.

grade: devel # must be 'stable' to release into candidate/stable channels
confinement: strict # use 'strict' once you have the right plugs and slots
compression: lzo

environment:
  # Set path to snap components
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION
  # To find shared libraries that are staged and moved to components
  ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR

hooks:
  install:
    plugs:
      # For hardware detection
      - hardware-observe
      - opengl
      - kernel-module-observe

parts:
  cli:
    source: https://github.com/canonical/inference-snaps-cli.git
    source-tag: v1.0.0-beta.24
    plugin: go
    build-snaps:
      - go/1.24/stable
    stage-packages:
      - pciutils
      - clinfo # for Intel GPU vRAM detection
    organize:
      bin/cli: bin/modelctl
  component-local-files:
    plugin: dump
    source: components
    organize:
      "llama-cpp": (component/llama-cpp)
      "model-30b-a3b-q4-k-m-gguf-1-of-6": (component/model-30b-a3b-q4-k-m-gguf-1-of-6)
      "model-30b-a3b-q4-k-m-gguf-2-of-6": (component/model-30b-a3b-q4-k-m-gguf-2-of-6)
      "model-30b-a3b-q4-k-m-gguf-3-of-6": (component/model-30b-a3b-q4-k-m-gguf-3-of-6)
      "model-30b-a3b-q4-k-m-gguf-4-of-6": (component/model-30b-a3b-q4-k-m-gguf-4-of-6)
      "model-30b-a3b-q4-k-m-gguf-5-of-6": (component/model-30b-a3b-q4-k-m-gguf-5-of-6)
      "model-30b-a3b-q4-k-m-gguf-6-of-6": (component/model-30b-a3b-q4-k-m-gguf-6-of-6)
  
  engines:
    source: engines
    plugin: dump
    organize:
      "*": engines/
  scripts:
    source: scripts
    plugin: dump
    organize:
      "*": bin/
  common-runtime-dependencies:
    plugin: nil
    stage-snaps:
      - jq # used in server.sh
  gemma-license:
    source: https://ai.google.dev/gemma/terms.md.txt
    source-type: file
    plugin: dump
    organize:
      "terms.md.txt": LICENSE_TERMS.md
  llama-cpp:
    plugin: dump
    source: 
      - on amd64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7731/llamacpp-amd64.tar.gz
      - on arm64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7731/llamacpp-arm64.tar.gz
    override-build: |
      # Move license files to be included in the snap, not the component
      mkdir -p $CRAFT_PRIME/usr/share/doc/
      mv licenses $CRAFT_PRIME/usr/share/doc/llama.cpp

      craftctl default
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llama-cpp)
  
  notice:
    plugin: nil
    source: NOTICE
    source-type: file
    override-build: |
      license_dir=$CRAFT_PART_INSTALL/usr/share/doc
      mkdir -p $license_dir
      cp NOTICE $license_dir/
      # Add license files referenced in NOTICE
      cp $SNAPCRAFT_PROJECT_DIR/LICENSE $license_dir/
      cp $SNAPCRAFT_PROJECT_DIR/nvidia-open-model-license-agreements-24-10-2025.pdf $license_dir/


components:
  model-30b-a3b-q4-k-m-gguf-1-of-6: &model
    type: standard
    summary: NVIDIA Nemotron-Nano-3-30B-A3B-Q4_K_M GGUF
    description: Model weights for NVIDIA Nemotron 3 Nano with 30B (3.5B active) parameters in GGUF format

  model-30b-a3b-q4-k-m-gguf-2-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-3-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-4-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-5-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-6-of-6:
    <<: *model

  llama-cpp:
    type: standard
    summary: llama.cpp HTTP server
    description: Optimized for inference on CPUs

apps:
  nemotron-snap:
    command: bin/modelctl
    plugs:
      # For hardware detection
      - hardware-observe
      - opengl
      # To access server over network socket
      - network
  server:
    command: bin/server.sh
    daemon: simple
    plugs:
      # Needed for server app
      - network-bind
      # For inference and when running some modelctl commands
      - hardware-observe
      - opengl

components: 
  nemotoron-nano3-30b:
    type: standard
    summary: NVIDIA Nemotron-Nano-3-30B-A3B-Q4_K_M GGUF
    description: 30B parameter LLM model in GGUF format optimized for CPU inference
  llama-cpp:
    type: standard
    summary: llama.cpp HTTP server
    description: Optimized for inference on CPUs
