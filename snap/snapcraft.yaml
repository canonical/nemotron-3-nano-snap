name: nemotron-3-nano
base: core24
summary: Inference Snap for Nemotron 3 nano
description: |
  This is an inference snap that lets you run Nemotron 3 nano.

adopt-info: version

grade: devel
confinement: strict
compression: lzo

environment:
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION
  ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR

hooks:
  install:
    plugs:
      - hardware-observe
      - opengl

parts:
  version:
    plugin: dump
    source: .
    build-packages:
      - git-lfs
    override-pull: |
      craftctl set version="v3+$(git -C $SNAPCRAFT_PROJECT_DIR describe --always --dirty)"

  cli:
    source: https://github.com/canonical/inference-snaps-cli.git
    source-tag: v1.0.0-beta.27
    plugin: go
    build-snaps:
      - go/1.24/stable
    stage-packages:
      - pciutils
      - clinfo
    organize:
      bin/cli: bin/modelctl

  component-local-files:
    plugin: dump
    source: components
    organize:
      "llama-cpp": (component/llama-cpp)
      "model-30b-a3b-q4-k-m-gguf-1-of-6": (component/model-30b-a3b-q4-k-m-gguf-1-of-6)
      "model-30b-a3b-q4-k-m-gguf-2-of-6": (component/model-30b-a3b-q4-k-m-gguf-2-of-6)
      "model-30b-a3b-q4-k-m-gguf-3-of-6": (component/model-30b-a3b-q4-k-m-gguf-3-of-6)
      "model-30b-a3b-q4-k-m-gguf-4-of-6": (component/model-30b-a3b-q4-k-m-gguf-4-of-6)
      "model-30b-a3b-q4-k-m-gguf-5-of-6": (component/model-30b-a3b-q4-k-m-gguf-5-of-6)
      "model-30b-a3b-q4-k-m-gguf-6-of-6": (component/model-30b-a3b-q4-k-m-gguf-6-of-6)
  
  engines:
    source: engines
    plugin: dump
    organize:
      "*": engines/
  scripts:
    source: scripts
    plugin: dump
    organize:
      "*": bin/

  common-runtime-dependencies:
    plugin: nil
    stage-snaps:
      - jq

  llama-cpp:
    plugin: dump
    source: 
      - on amd64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7731/llamacpp-amd64.tar.gz
      - on arm64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7731/llamacpp-arm64.tar.gz
    override-build: |
      # Move license files to be included in the snap, not the component
      mkdir -p $CRAFT_PRIME/usr/share/doc/
      mv licenses $CRAFT_PRIME/usr/share/doc/llama.cpp

      craftctl default
    stage-packages:
      - libgomp1
    organize:
      "*": (component/llama-cpp)
  
  notice:
    plugin: nil
    source: NOTICE
    source-type: file
    override-build: |
      license_dir=$CRAFT_PART_INSTALL/usr/share/doc
      mkdir -p $license_dir
      cp NOTICE $license_dir/
      # Add license files referenced in NOTICE
      cp $SNAPCRAFT_PROJECT_DIR/LICENSE $license_dir/
      cp $SNAPCRAFT_PROJECT_DIR/nvidia-open-model-license-agreements-24-10-2025.pdf $license_dir/


components:
  model-30b-a3b-q4-k-m-gguf-1-of-6: &model
    type: standard
    summary: NVIDIA Nemotron-Nano-3-30B-A3B-Q4_K_M GGUF
    description: Model weights for NVIDIA Nemotron 3 Nano with 30B (3.5B active) parameters in GGUF format

  model-30b-a3b-q4-k-m-gguf-2-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-3-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-4-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-5-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-6-of-6:
    <<: *model

  llama-cpp:
    type: standard
    summary: llama.cpp HTTP server
    description: Optimized for inference on CPUs

apps:
  nemotron-snap:
    command: bin/modelctl
    plugs:
      # For hardware detection
      - hardware-observe
      - opengl
      # To access server over network socket
      - network
  server:
    command: bin/server.sh
    daemon: simple
    plugs:
      # Needed for server app
      - network-bind
      # For inference and when running some modelctl commands
      - hardware-observe
      - opengl

components:
  model-30b-a3b-q4-k-m-gguf-1-of-6: &model
    type: standard
    summary: NVIDIA Nemotron-Nano-3-30B-A3B-Q4_K_M GGUF
    description: Model weights for NVIDIA Nemotron 3 Nano with 30B (3.5B active) parameters in GGUF format

  model-30b-a3b-q4-k-m-gguf-2-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-3-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-4-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-5-of-6:
    <<: *model

  model-30b-a3b-q4-k-m-gguf-6-of-6:
    <<: *model

  llama-cpp:
    type: standard
    summary: llama.cpp HTTP server
    description: Optimized for inference on CPUs

apps:
  nemotron-3-nano:
    command: bin/modelctl
    plugs:
      - hardware-observe
      - opengl
      - network

  server:
    command: bin/server.sh
    daemon: simple
    plugs:
      - network-bind
      - hardware-observe
      - opengl

